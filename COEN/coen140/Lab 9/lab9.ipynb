{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "This lab demonstrates two different types of ensemble methods, bagging and boosting. Recall that some algorithms (decision trees in this activity) can suffer from high variance. Their results can vary drastically depending on the training data provided. Therefore, the general concept is to build multiple models with multiple subsets of the data to yield a more stable and accurate solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: What is Bootstrap Sampling?\n",
    "\n",
    "From a statistical point of view, bootstrap sampling is a method that involves sampling data repeatedly with replacement from a data source to estimate a population parameter. To break this down further: <br />\n",
    "**Sampling** - the process of selecting a subset of items from a vast collection of items in order to estimate a certain characterstic <br />\n",
    "**Sampling with replacement** - A data point that is drawn can reappear in future drawn samples. The data point it is not removed from the data source <br />\n",
    "**Parameter Estimation** - A parameter is some measurable characteristic associated with a given population. This could be something like mean, standard deviation, or from a machine learning perspective, some sort of learned coefficient. <br />\n",
    "\n",
    "In statistics this is done because sampling an entire population is infeasible. Instead, small random samples are taken and then averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do This**: In the cell below, we compute the average of 100 random numbers. Instead of computing this average directly, use the `resample` function in `sklearn.utils` to sample 10% of the data 20, 100, and 1000 times. Each time you take a 10% sample, store the average of those numbers, then report the average of all the averages you stored for each experiment (repeating 20 times, 100 times, and 1000 times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual average: 0.5124\n",
      "Estimated Average with 20 iterations: 0.5281\n",
      "Estimated Average with 100 iterations: 0.5124\n",
      "Estimated Average with 1000 iterations: 0.5130\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "size = 100\n",
    "randNums = np.zeros(size)\n",
    "\n",
    "# generate some random numbers\n",
    "seed(1)\n",
    "for i in range(0, size):\n",
    "    randNums[i] = random()\n",
    "\n",
    "actual_avg = np.average(randNums)\n",
    "print('Actual average: {:.4f}'.format(actual_avg))\n",
    "\n",
    "# Sample 10% of data (20 times) and take the mean\n",
    "estimated_avg = 0\n",
    "for i in range(20):\n",
    "    sample = resample(randNums, n_samples = 10, replace = False)\n",
    "    avg = np.average(sample)\n",
    "    estimated_avg += avg\n",
    "    \n",
    "estimated_avg = estimated_avg/20\n",
    "\n",
    "\n",
    "print('Estimated Average with 20 iterations: {:.4f}'.format(estimated_avg))\n",
    "\n",
    "# Sample 10% of data (100 times) and take the mean\n",
    "estimated_avg = 0\n",
    "for i in range(100):\n",
    "    sample = resample(randNums, n_samples = 10, replace = False)\n",
    "    avg = np.average(sample)\n",
    "    estimated_avg += avg\n",
    "    \n",
    "estimated_avg = estimated_avg/100\n",
    "\n",
    "\n",
    "\n",
    "print('Estimated Average with 100 iterations: {:.4f}'.format(estimated_avg))\n",
    "\n",
    "# Sample 10% of data (1000 times) and take the mean\n",
    "estimated_avg = 0\n",
    "for i in range(1000):\n",
    "    sample = resample(randNums, n_samples = 10, replace = False)\n",
    "    avg = np.average(sample)\n",
    "    estimated_avg += avg\n",
    "    \n",
    "estimated_avg = estimated_avg/1000\n",
    "\n",
    "\n",
    "\n",
    "print('Estimated Average with 1000 iterations: {:.4f}'.format(estimated_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of iterations increases, the estimated average will converge to the actual average. In other words, the variance of the estimated average decreases as the number of iterations increases. This is the fundamental idea behind bootstrapping and the driving principle behind the next few exercises below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Bootstrap Aggregating (Bagging)\n",
    "\n",
    "Bagging is an ensemble method that takes the idea of bootstrapping by combining predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model. The benefit of this is a reduction in variance and therefore an increase in prediction accuracy. <br /> \n",
    "For the next few examples we will use the Sonar dataset from the UCI Machine Learning repository. This is a dataset that describes sonar chirp returns bouncing off of different surfaces. The 60 input variables are the strength of the returns at different angles. It is a binary classification problem that requires a model to differentiate rocks from metal cylinders. There are 208 observations with \"M\" representing mine and \"R\" representing rocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sonar Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "         9   ...      51      52      53      54      55      56      57  \\\n",
       "0    0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2    0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3    0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4    0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.2684  ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
       "204  0.2154  ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
       "205  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
       "206  0.2354  ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
       "207  0.2354  ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
       "\n",
       "         58      59  60  \n",
       "0    0.0090  0.0032   1  \n",
       "1    0.0052  0.0044   1  \n",
       "2    0.0095  0.0078   1  \n",
       "3    0.0040  0.0117   1  \n",
       "4    0.0107  0.0094   1  \n",
       "..      ...     ...  ..  \n",
       "203  0.0193  0.0157   0  \n",
       "204  0.0062  0.0067   0  \n",
       "205  0.0077  0.0031   0  \n",
       "206  0.0036  0.0048   0  \n",
       "207  0.0061  0.0115   0  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "def LoadData(sfile):\n",
    "    # Load samples\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=sfile, \n",
    "        header=None)\n",
    "    return df\n",
    "\n",
    "def ParseData(df):\n",
    "    # Convert mine and rock label to integers\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df[60])\n",
    "    df[60] = le.transform(df[60])\n",
    "    labels = df.iloc[:,60].values\n",
    "    df = df.drop([60], axis=1)\n",
    "    samples = df.iloc[:,:].values\n",
    "    return samples, labels\n",
    "\n",
    "df = LoadData('data/sonar.all-data.csv')\n",
    "[samples, labels] = ParseData(df)\n",
    "print('Sonar Data')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that utilizes cross validation to test accuracy of model\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "def evaluate_model(model):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, samples, labels, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print('Accuracy: {:.4f}'.format(scores.mean()))\n",
    "    print('Cross validation accuracies')\n",
    "    print(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do This**: In the two empty cells below, compare the `DecisionTreeClassifier` and the `BaddingClassifier` models using the `evaluate_model` function above. Set `random_state` to 0 for both models and `n_estimators` to 100 for the `BaddingClassifier` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classic Decision Tree - no ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7240\n",
      "Cross validation accuracies\n",
      "[0.80952381 0.66666667 0.76190476 0.61904762 0.76190476 0.76190476\n",
      " 0.71428571 0.71428571 0.7        0.7        0.61904762 0.71428571\n",
      " 0.76190476 0.66666667 0.71428571 0.85714286 0.76190476 0.71428571\n",
      " 0.65       0.75       0.66666667 0.76190476 0.57142857 0.80952381\n",
      " 0.80952381 0.80952381 0.76190476 0.76190476 0.75       0.6       ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def evaluate_model(model):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, samples, labels, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print('Accuracy: {:.4f}'.format(scores.mean()))\n",
    "    print('Cross validation accuracies')\n",
    "    print(scores)\n",
    "    return scores\n",
    "\n",
    "from sklearn import preprocessing\n",
    "def LoadData(sfile):\n",
    "    # Load samples\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=sfile, \n",
    "        header=None)\n",
    "    return df\n",
    "\n",
    "def ParseData(df):\n",
    "    # Convert mine and rock label to integers\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df[60])\n",
    "    df[60] = le.transform(df[60])\n",
    "    labels = df.iloc[:,60].values\n",
    "    df = df.drop([60], axis=1)\n",
    "    samples = df.iloc[:,:].values\n",
    "    return samples, labels\n",
    "\n",
    "df = LoadData('data/sonar.all-data')\n",
    "[samples, labels] = ParseData(df)\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "score = evaluate_model(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bagged Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8045\n",
      "Cross validation accuracies\n",
      "[0.76190476 0.80952381 0.80952381 0.71428571 0.66666667 0.85714286\n",
      " 0.9047619  0.85714286 0.85       0.85       0.85714286 0.80952381\n",
      " 0.9047619  0.71428571 0.71428571 0.85714286 0.76190476 0.85714286\n",
      " 0.8        0.85       0.71428571 0.85714286 0.66666667 0.85714286\n",
      " 0.76190476 0.9047619  0.80952381 0.85714286 0.85       0.65      ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "def evaluate_model(model):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, samples, labels, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print('Accuracy: {:.4f}'.format(scores.mean()))\n",
    "    print('Cross validation accuracies')\n",
    "    print(scores)\n",
    "    return scores\n",
    "\n",
    "from sklearn import preprocessing\n",
    "def LoadData(sfile):\n",
    "    # Load samples\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=sfile, \n",
    "        header=None)\n",
    "    return df\n",
    "\n",
    "def ParseData(df):\n",
    "    # Convert mine and rock label to integers\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df[60])\n",
    "    df[60] = le.transform(df[60])\n",
    "    labels = df.iloc[:,60].values\n",
    "    df = df.drop([60], axis=1)\n",
    "    samples = df.iloc[:,:].values\n",
    "    return samples, labels\n",
    "\n",
    "df = LoadData('data/sonar.all-data')\n",
    "[samples, labels] = ParseData(df)\n",
    "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=0)\n",
    "score = evaluate_model(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** <br/>\n",
    "There are several hyperparameters for a decision tree that have not been optimized in the above example. These include things such as max depth, miniminum number of leaf samples, or class purity. Even with these optimized, a bagged decision tree will generally outperform a single tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Random Forest\n",
    "\n",
    "The issue with bagged decision trees is that each tree contains the same number of features. As a result, while many trees are produced, it turns out they are highly correlated with each other. This problem becomes exacerbated when there are dominant features that create the best class purity, and therefore are always chosen as the splitting point. To combat this, we shall pick a random subset of features, diversifying each tree and reducing structural similarity between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Random Forest\n",
    "\n",
    "The sklearn library has a random forest function which has defaults for a handful of hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8265\n",
      "Cross validation accuracies\n",
      "[0.80952381 0.9047619  0.80952381 0.71428571 0.71428571 0.80952381\n",
      " 0.95238095 0.95238095 0.9        0.7        0.9047619  0.9047619\n",
      " 0.85714286 0.71428571 0.80952381 0.9047619  0.85714286 0.85714286\n",
      " 0.75       0.75       0.80952381 0.76190476 0.76190476 0.80952381\n",
      " 0.85714286 0.9047619  0.85714286 0.85714286 0.85       0.75      ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "scores = evaluate_model(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Random Forest\n",
    "There is some hyperparameter tuning that can be done here. Some of which include number of bootstrap samples, number of features, number of trees, or tree depth. Generally when using random forests the trees are grown to their entirety without pruning which, counterintuitively, does not cause overfitting. Therefore the tuning of these hyperparameters might only cause very marginal gains at best. Below we will experiment with the number of features.\n",
    "\n",
    "**Do This**: Experiment with setting `max_features` to values in the range $[1,16]$ in a `RandomForestClassifier` model. For each model, set `random_state` to 0 and print the average accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features =  1\n",
      "Accuracy: 0.8379\n",
      "max_features =  2\n",
      "Accuracy: 0.8329\n",
      "max_features =  3\n",
      "Accuracy: 0.8318\n",
      "max_features =  4\n",
      "Accuracy: 0.8252\n",
      "max_features =  5\n",
      "Accuracy: 0.8410\n",
      "max_features =  6\n",
      "Accuracy: 0.8218\n",
      "max_features =  7\n",
      "Accuracy: 0.8265\n",
      "max_features =  8\n",
      "Accuracy: 0.8219\n",
      "max_features =  9\n",
      "Accuracy: 0.8250\n",
      "max_features =  10\n",
      "Accuracy: 0.8475\n",
      "max_features =  11\n",
      "Accuracy: 0.8332\n",
      "max_features =  12\n",
      "Accuracy: 0.8139\n",
      "max_features =  13\n",
      "Accuracy: 0.8103\n",
      "max_features =  14\n",
      "Accuracy: 0.8217\n",
      "max_features =  15\n",
      "Accuracy: 0.8074\n",
      "max_features =  16\n",
      "Accuracy: 0.8205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def evaluate_model(model):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, samples, labels, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print('Accuracy: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "for i in range(1, 17):\n",
    "    print(\"max_features = \", i)\n",
    "    rf = RandomForestClassifier(random_state=0, max_features = i)\n",
    "    scores = evaluate_model(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this shows is that there might be a better selection of the number of features rather than the default  $\\sqrt{m}$ where m is the number of features, in this dataset, 60. However, caution should be excercised here as the random state of the classifier is set to 0. Changing this will lead to different results so more experimentation would need to be performed to justify changing the default number of features. <br/>\n",
    "\n",
    "**Note** <br/>\n",
    "- As the Bagged decision trees are constructed, we can calculate how much the error function drops for a variable at each split point. <br/>\n",
    "- In regression problems this may be the drop in sum squared error and in classification this might be the Gini score. <br/>\n",
    "- These drops in error can be averaged across all decision trees and output to provide an estimate of the importance of each input variable. The greater the drop when the variable was chosen, the greater the importance. <br/>\n",
    "- These outputs can help identify subsets of input variables that may be most or least relevant to the problem and suggest at possible feature selection experiments you could perform where some features are removed from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Boosting\n",
    "\n",
    "Boosting ensemble algorithms create a sequence of models that attempt to correct the mistakes of the models before them in the sequence. Once created, the models make predictions which may be weighted by their demonstrated accuracy and the results are combined to create a final output prediciton.  \n",
    "\n",
    "A couple things to be aware of <br/>\n",
    "**Outliers** - Outliers will force the ensemble down the rabbit hole of working hard to correct for cases that are unrealistic. These should be removed prior to training. <br/>\n",
    "**Quality Labels** - Because the ensemble method attempts to correct misclassifications in the training data, you need to be careful that the data is labeled as accurately as possible. <br/>\n",
    "\n",
    "We will explore a boosting method called **Adaptive Boosting (AdaBoost)** <br/>\n",
    "AdaBoost was the first really successful boosting algorithm developed for binary classification. AdaBoost can be used to boost the performance of any machine learning algorithm. It is best used with weak learners. These are models that achieve accuracy just above random chance on a classification problem.The most suited and therefore most common algorithm used with AdaBoost are decision trees with one level. Because these trees are so short and only contain one decision for classification, they are often called decision stumps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default AdaBoost\n",
    "\n",
    "Just like Random Forests, the sklearn library has an AdaBoost function which has defaults for a handful of hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do This**: Use the `evaluate_model` function to evalutate the `AdaBoostClassifier` with `random_state` set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8218\n",
      "Cross validation accuracies\n",
      "[0.85714286 0.95238095 0.85714286 0.76190476 0.80952381 0.95238095\n",
      " 0.9047619  0.85714286 0.75       0.85       0.76190476 0.80952381\n",
      " 0.9047619  0.66666667 0.76190476 0.76190476 0.85714286 0.80952381\n",
      " 0.85       0.7        0.85714286 0.85714286 0.66666667 0.80952381\n",
      " 0.80952381 0.95238095 0.85714286 0.80952381 0.9        0.7       ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def evaluate_model(model):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, samples, labels, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print('Accuracy: {:.4f}'.format(scores.mean()))\n",
    "    print('Cross validation accuracies')\n",
    "    print(scores)\n",
    "    return scores\n",
    "\n",
    "clf = AdaBoostClassifier(random_state=0)\n",
    "score = evaluate_model(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning AdaBoost\n",
    "There is some hyperparameter tuning that can be done here. Some of which include number of trees, depth of weak learner, learning rate (contribution of each model to ensemble prediction), and even different weak learners such as KNN. Below we will experiement with number of trees and depth of the weak learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do This**: Tune the number of trees (`n_estimators`) parameter in `AdaBoostClassifier`, testing values in the set $\\{10,50,100,500,1000,5000\\}$. For each model print the average Accuracy for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Accuracy: 0.7677\n",
      "50\n",
      "Accuracy: 0.8218\n",
      "100\n",
      "Accuracy: 0.8410\n",
      "500\n",
      "Accuracy: 0.8456\n",
      "1000\n",
      "Accuracy: 0.8540\n",
      "5000\n",
      "Accuracy: 0.8507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def evaluate_model(model):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, samples, labels, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print('Accuracy: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "arr = [10, 50, 100, 500, 1000, 5000]\n",
    "for i in arr:\n",
    "    print(i)\n",
    "    clf = AdaBoostClassifier(random_state=0, n_estimators = i)\n",
    "    score = evaluate_model(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do This**: Tune the maximum depth (`max_depth`) parameter in a `DecisionTreeClassifier` which is used as the `base_estimator` for an `AdaBoostClassifier` model, testing values in the range $[1,7]$. For each model, print the average Accuracy for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Accuracy: 0.8218\n",
      "2\n",
      "Accuracy: 0.8130\n",
      "3\n",
      "Accuracy: 0.8013\n",
      "4\n",
      "Accuracy: 0.8414\n",
      "5\n",
      "Accuracy: 0.8350\n",
      "6\n",
      "Accuracy: 0.7486\n",
      "7\n",
      "Accuracy: 0.7311\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "for i in range(1, 8):\n",
    "    print(i)\n",
    "    x = DecisionTreeClassifier(max_depth = i)\n",
    "    clf = AdaBoostClassifier(random_state=0, base_estimator = x)\n",
    "    score = evaluate_model(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hyperparameter tuning shows some promising results. The accuracy can be imporoved on this dataset if the number of trees and depth is adjusted. Further progress might be possible if other parameters are tuned. As mentioned before, the seed is set to 0 so be careful to insure that these adjustments are true when the stochastic nature of the algorithm isn't fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1.) https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/ <br/>\n",
    "2.) https://machinelearningmastery.com/random-forest-ensemble-in-python/ <br/>\n",
    "3.) https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/ <br/>\n",
    "4.) https://machinelearningmastery.com/implement-bagging-scratch-python/ <br/>\n",
    "5.) https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/ <br/>\n",
    "6.) https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/ <br/>\n",
    "7.) https://www.analyticsvidhya.com/blog/2020/02/what-is-bootstrap-sampling-in-statistics-and-machine-learning/ <br/>\n",
    "8.) https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/ <br/>\n",
    "9.) https://machinelearningmastery.com/adaboost-ensemble-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
